{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "703ae6ca-dd25-44e8-8926-8ef80c1a4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7aac628-876b-4dee-8821-ffa2775fcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class CXRSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, lungs_dir, heart_dir,\n",
    "                 size=(512,512),\n",
    "                 augment=False):\n",
    "        \"\"\"\n",
    "        img_dir   : folder of resized grayscale JPEGs of original X‑rays\n",
    "        lungs_dir : folder of resized lungs masks (0/255 PNGs)\n",
    "        heart_dir : folder of resized heart masks (0/255 PNGs)\n",
    "        size      : (width, height) to resize to\n",
    "        augment   : whether to apply random flips/rotations\n",
    "        \"\"\"\n",
    "        # collect IDs from the .jpg files in img_dir\n",
    "        self.ids = [os.path.splitext(f)[0]\n",
    "                    for f in os.listdir(img_dir)\n",
    "                    if f.lower().endswith(\".jpg\")]\n",
    "        print(f\"Found {len(self.ids)} samples in {img_dir}\")\n",
    "        \n",
    "        self.img_dir   = img_dir\n",
    "        self.lungs_dir = lungs_dir\n",
    "        self.heart_dir = heart_dir\n",
    "\n",
    "        # image transformations\n",
    "        self.tf_img = transforms.Compose([\n",
    "            transforms.Resize(size),\n",
    "            transforms.ToTensor(),                         # [1,H,W], floats in [0,1]\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])    # adjust to your data\n",
    "        ])\n",
    "        # mask resizing (nearest to preserve labels)\n",
    "        self.tf_mask = transforms.Resize(size,\n",
    "                                         interpolation=transforms.InterpolationMode.NEAREST)\n",
    "\n",
    "        # optional augmentations\n",
    "        self.aug = transforms.RandomChoice([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "        ]) if augment else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _id = self.ids[idx]\n",
    "\n",
    "        img_path   = os.path.join(self.img_dir,   f\"{_id}.jpg\")\n",
    "        lung_path  = os.path.join(self.lungs_dir, f\"{_id}_lungs.png\")\n",
    "        heart_path = os.path.join(self.heart_dir, f\"{_id}_heart.png\")\n",
    "\n",
    "        img    = Image.open(img_path).convert(\"L\")\n",
    "        mask_l = Image.open(lung_path)\n",
    "        mask_h = Image.open(heart_path)\n",
    "\n",
    "        # Apply same random augmentation to all three\n",
    "        if self.aug:\n",
    "            seed = np.random.randint(0, 1_000_000)\n",
    "            torch.manual_seed(seed)\n",
    "            img    = self.aug(img)\n",
    "            torch.manual_seed(seed)\n",
    "            mask_l = self.aug(mask_l)\n",
    "            torch.manual_seed(seed)\n",
    "            mask_h = self.aug(mask_h)\n",
    "\n",
    "        # Resize\n",
    "        img    = self.tf_img(img)         # tensor [1,H,W]\n",
    "        mask_l = self.tf_mask(mask_l)     # PIL image resized\n",
    "        mask_h = self.tf_mask(mask_h)\n",
    "\n",
    "        # Convert masks to tensors [1,H,W] uint8\n",
    "        mask_l = transforms.PILToTensor()(mask_l)\n",
    "        mask_h = transforms.PILToTensor()(mask_h)\n",
    "\n",
    "        # Build a single multi-class mask: 0=BG,1=Lung,2=Heart\n",
    "        ml = (mask_l.squeeze(0) // 255).to(torch.uint8)\n",
    "        mh = (mask_h.squeeze(0) // 255).to(torch.uint8)\n",
    "        mask = torch.zeros_like(ml, dtype=torch.uint8)\n",
    "        mask[ml == 1] = 1\n",
    "        mask[mh == 1] = 2\n",
    "\n",
    "        return img, mask.long()  # img: [1,H,W], mask: [H,W] ints in {0,1,2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a652b852-08df-43b9-b762-930de48ff4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# — Attention Block —\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        # gating signal F_g, skip connection F_l\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "# — Double Convolution block —\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "# — Attention U‑Net —\n",
    "class AttUNet(nn.Module):\n",
    "    def __init__(self, n_channels=1, n_classes=3, features=[64,128,256,512]):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.downs = nn.ModuleList()\n",
    "        for f_in, f_out in zip([n_channels]+features, features):\n",
    "            self.downs.append(DoubleConv(f_in, f_out))\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        # Decoder with Attention\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.attentions = nn.ModuleList()\n",
    "        rev_features = features[::-1]\n",
    "        for f in rev_features:\n",
    "            self.ups.append(nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2))\n",
    "            self.attentions.append(AttentionBlock(F_g=f, F_l=f, F_int=f//2))\n",
    "            self.ups.append(DoubleConv(f*2, f))\n",
    "        # Final conv\n",
    "        self.final_conv = nn.Conv2d(features[0], n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            up_trans = self.ups[idx]\n",
    "            attn     = self.attentions[idx//2]\n",
    "            conv     = self.ups[idx+1]\n",
    "            x = up_trans(x)\n",
    "            skip = skip_connections[idx//2]\n",
    "            # crop if needed to match sizes\n",
    "            if x.shape != skip.shape:\n",
    "                x = F.interpolate(x, size=skip.shape[2:])\n",
    "            # attention & concatenation\n",
    "            skip = attn(g=x, x=skip)\n",
    "            x = torch.cat([skip, x], dim=1)\n",
    "            x = conv(x)\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25743e02-9078-47cd-9145-dc209218b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 253 samples in processed_images\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# assuming you have CXRSegmentationDataset defined\n",
    "train_ds = CXRSegmentationDataset(\n",
    "    img_dir   = \"processed_images\",\n",
    "    lungs_dir = \"processed_masks/lungs\",\n",
    "    heart_dir = \"processed_masks/heart\",\n",
    "    size      = (512,512),\n",
    "    augment   = True\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86a4fce2-afbe-4e1b-9fb7-da93eec7b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a699713-fb41-4e0f-9c65-6de0c372b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0   # <— no multiprocessing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c186d0a-b03a-4567-82cf-b2eb5ea47575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attunet-batch-run</strong> at: <a href='https://wandb.ai/park11871247-new-york-university/cxr-segmentation/runs/cgx1htbb' target=\"_blank\">https://wandb.ai/park11871247-new-york-university/cxr-segmentation/runs/cgx1htbb</a><br> View project at: <a href='https://wandb.ai/park11871247-new-york-university/cxr-segmentation' target=\"_blank\">https://wandb.ai/park11871247-new-york-university/cxr-segmentation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250420_210522-cgx1htbb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/chris/Downloads/Advanced_Topics Project/CheXmask-Database/wandb/run-20250420_210904-to2ni09m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/park11871247-new-york-university/cxr-segmentation/runs/to2ni09m' target=\"_blank\">attunet-batch-run</a></strong> to <a href='https://wandb.ai/park11871247-new-york-university/cxr-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/park11871247-new-york-university/cxr-segmentation' target=\"_blank\">https://wandb.ai/park11871247-new-york-university/cxr-segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/park11871247-new-york-university/cxr-segmentation/runs/to2ni09m' target=\"_blank\">https://wandb.ai/park11871247-new-york-university/cxr-segmentation/runs/to2ni09m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/park11871247-new-york-university/cxr-segmentation/runs/to2ni09m?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1277349d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0) set your W&B key in‑script\n",
    "os.environ[\"WANDB_API_KEY\"] = \"9b3cc6b608bb679c5cc822e2e256c754cc777ee0\"\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"], force=True)\n",
    "\n",
    "# 1) init W&B\n",
    "wandb.init(\n",
    "    project=\"cxr-segmentation\",\n",
    "    name=\"attunet-batch-run\",\n",
    "    config={\n",
    "        \"epochs\": 15,\n",
    "        \"batch_size\": 8,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"architecture\": \"Attention U-Net\",\n",
    "        \"input_size\": [512,512],\n",
    "        \"n_classes\": 3\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f69838-c93e-459a-b5c5-1e2c4bf2b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model     = AttUNet(n_channels=1, n_classes=wandb.config.n_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# 4) watch gradients & parameters\n",
    "wandb.watch(model, log=\"all\", log_freq=50)\n",
    "\n",
    "# 5) training loop\n",
    "for epoch in range(wandb.config.epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, masks in train_loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{wandb.config.epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # log to W&B\n",
    "    wandb.log({\n",
    "        \"epoch\":      epoch+1,\n",
    "        \"train_loss\": epoch_loss\n",
    "    })\n",
    "\n",
    "# 6) save model artifact\n",
    "torch.save(model.state_dict(), \"attunet_final.pt\")\n",
    "wandb.save(\"attunet_final.pt\")\n",
    "\n",
    "# 7) finish\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
